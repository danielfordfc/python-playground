spark.streaming.kafka.allowNonConsecutiveOffsets=true
spark.streaming.kafka.consumer.poll.ms=1000
spark.executor.cores=1

# hoodie only support org.apache.spark.serializer.KryoSerializer as spark.serializer
spark.serializer=org.apache.spark.serializer.KryoSerializer

# In Spark 3.0 and before Spark uses KafkaConsumer for offset fetching which
# could cause infinite wait in the driver. In Spark 3.1 a new configuration option
# added spark.sql.streaming.kafka.useDeprecatedOffsetFetching (default: true)
# which could be set to false allowing Spark to use new offset fetching mechanism
# using AdminClient.

# First of all the new approach supports Kafka brokers 0.11.0.0+.

spark.sql.streaming.kafka.useDeprecatedOffsetFetching=false

spark.parquet.avro.write-old-list-structure=false

# iceberg configs
# spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions
# spark.sql.catalog.spark_catalog=org.apache.iceberg.spark.SparkSessionCatalog
# spark.sql.catalog.spark_catalog.type=hive
# spark.sql.catalog.local=org.apache.iceberg.spark.SparkCatalog
# spark.sql.catalog.local.type=hadoop
# spark.sql.catalog.local.warehouse=$base_path
# spark.sql.defaultCatalog=local

# # Spark Properties
# spark.driver.extraJavaOptions="-XX:+PrintGCApplicationStoppedTime -XX:+PrintGCApplicationConcurrentTime -XX:+PrintGCTimeStamps -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/varadarb_ds_driver.hprof" \
# spark.executor.extraJavaOptions="-XX:+PrintGCApplicationStoppedTime -XX:+PrintGCApplicationConcurrentTime -XX:+PrintGCTimeStamps -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/tmp/varadarb_ds_executor.hprof" \
#  # --queue hadoop-platform-queue \
# spark.scheduler.mode=FAIR \
# spark.yarn.executor.memoryOverhead=1072 \
# spark.yarn.driver.memoryOverhead=2048 \
# spark.task.cpus=1 \
# spark.executor.cores=1 \
# spark.task.maxFailures=10 \
# spark.memory.fraction=0.4 \
# spark.rdd.compress=true \
# spark.kryoserializer.buffer.max=200m \
# spark.serializer=org.apache.spark.serializer.KryoSerializer \
# spark.memory.storageFraction=0.1 \
# spark.shuffle.service.enabled=true \
# spark.sql.hive.convertMetastoreParquet=false \
# spark.ui.port=5555 \
# spark.driver.maxResultSize=3g \
# spark.executor.heartbeatInterval=120s \
# spark.network.timeout=600s \
# spark.eventLog.overwrite=true \
# spark.eventLog.enabled=true \
# spark.eventLog.dir=/tmp/spark-events \
# spark.yarn.max.executor.failures=10 \
# spark.sql.catalogImplementation=hive \
# spark.sql.shuffle.partitions=100
