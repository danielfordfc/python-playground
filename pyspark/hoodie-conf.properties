# All --hoodie-conf properties can go here

# We allow duplicates on inserts!
hoodie.merge.allow.duplicate.on.inserts=true

# Key fields, for kafka example
hoodie.datasource.write.recordkey.field=viewtime
hoodie.datasource.write.partitionpath.field=pageid

# Schema provider props (change to absolute path based on your installation)
# hoodie.deltastreamer.schemaprovider.source.schema.file=/~/Sandbox/spark-sandbox/src/main/resources/pageviews.avsc
# hoodie.deltastreamer.schemaprovider.target.schema.file=/~/Sandbox/spark-sandbox/src/main/resources/pageviews.avsc
# Kafka Source
hoodie.deltastreamer.source.kafka.topic=${topic}
# hoodie.deltastreamer.source.kafka.value.deserializer.class=org.apache.hudi.utilities.deser.KafkaAvroSchemaDeserializer
hoodie.deltastreamer.schemaprovider.registry.url=http://localhost:8081/subjects/${topic}-value/versions/latest

hoodie.deltastreamer.source.kafka.append.offsets=true

# Kafka Consumer props
bootstrap.servers=localhost:9092
auto.offset.reset=earliest
schema.registry.url=http://localhost:8081
# Consumer Group
group.id=hudi-deltastreamer-${topic}


isolation.level=read_committed

# We don't want Consumer to periodically commit offsets because Spark Streaming/Hudi may fail to write the successfully polled records from Kafka
enable.auto.commit=false
# max.poll.records=8

# hoodie.merge.allow.duplicate.on.inserts=true
# hoodie.datasource.write.recordkey.field=command_id
# hoodie.datasource.write.precombine.field=requested_at
# hoodie.datasource.write.keygenerator.class=org.apache.hudi.keygen.NonpartitionedKeyGenerator
# hoodie.datasource.hive_sync.partition_extractor_class=org.apache.hudi.hive.NonPartitionedExtractor
# hoodie.datasource.write.partitionpath.field=''
# hoodie.deltastreamer.source.kafka.value.deserializer.class=org.apache.hudi.utilities.deser.KafkaAvroSchemaDeserializer
# hoodie.datasource.hive_sync.enable=true
# hoodie.datasource.hive_sync.database=kirby_example_raw
# hoodie.datasource.hive_sync.table=make_credit_decision_1
# hoodie.datasource.hive_sync.partition_fields=''
# hoodie.deltastreamer.source.kafka.topic=make-credit-decision-1

# hoodie.database.name=kirby_example_raw
# hoodie.table.name=make_credit_decision_1

hoodie.datasource.write.keygenerator.class=org.apache.hudi.keygen.NonpartitionedKeyGenerator
hoodie.datasource.hive_sync.partition_extractor_class=org.apache.hudi.hive.NonPartitionedExtractor
# hoodie.datasource.write.partitionpath.field='',
# hoodie.datasource.hive_sync.partition_fields='',

# Spark Configurations Does not Work in this file
# spark.streaming.kafka.allowNonConsecutiveOffsets=true
# should set these via --properties-file for Spark, not as --props for hudi
