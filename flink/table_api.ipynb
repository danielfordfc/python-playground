{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyflink.common.configuration.Configuration at 0x158cf2940>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyflink.table import EnvironmentSettings, TableEnvironment\n",
    "import os\n",
    "\n",
    "# Create a batch TableEnvironment\n",
    "env_settings = EnvironmentSettings.in_batch_mode()\n",
    "table_env = TableEnvironment.create(env_settings)\n",
    "\n",
    "# Get the current working directory\n",
    "CURRENT_DIR = os.getcwd()\n",
    "\n",
    "# Define a list of JAR file names you want to add\n",
    "jar_files = [\n",
    "    \"flink-s3-fs-hadoop-1.17.2.jar\",\n",
    "    \"hudi-flink1.17-bundle-0.14.1.jar\",\n",
    "    \"flink-sql-connector-kafka-1.17.2.jar\"\n",
    "]\n",
    "\n",
    "# Build the list of JAR URLs by prepending 'file:///' to each file name\n",
    "jar_urls = [f\"file:///{CURRENT_DIR}/{jar_file}\" for jar_file in jar_files]\n",
    "\n",
    "\n",
    "table_env.get_config().get_configuration().set_string(\n",
    "    \"pipeline.jars\",\n",
    "    \";\".join(jar_urls),\n",
    ")\n",
    "\n",
    "table_env.get_config().get_configuration().set_string(\n",
    "    \"AWS_ACCESS_KEY_ID\",\n",
    "    \"AKIAVS5IC62KWW2KWUQ4\"\n",
    ")\n",
    "\n",
    "table_env.get_config().get_configuration().set_string(\n",
    "    \"AWS_SECRET_ACCESS_KEY\",\n",
    "    \"AZU3lHXaPFtw/gR23s8Hr+voIEswcrbS7triQnva\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faker import Faker\n",
    "\n",
    "import uuid\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "\n",
    "# Generate fake data and convert it into a PyFlink table with column names\n",
    "data = [(str(uuid.uuid4()), fake.name(), fake.city(), fake.state()) for _ in range(10)]  # Generate 10 rows of fake data with UUIDs and names\n",
    "\n",
    "# Define column names\n",
    "column_names = [\"uuid\", \"first_name\", \"city\", \"state\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PyFlink table with column names\n",
    "table = table_env.from_elements(data, schema=column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "table_env.create_temporary_view('test_1', table) # this needs to be a unique name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+--------------------------------+--------------------------------+--------------------------------+\n",
      "|                           uuid |                     first_name |                           city |                          state |\n",
      "+--------------------------------+--------------------------------+--------------------------------+--------------------------------+\n",
      "| 9f13aa25-f737-4730-b700-782... |                 Michael Martin |                     West Carla |                      Tennessee |\n",
      "| a1dafb57-1ab2-4373-8958-00f... |                   George Adams |                North Emilyfurt |                       Oklahoma |\n",
      "| d8cded8f-a968-434a-bcf2-cbf... |                    Tara Martin |                  Port Patricia |                       Delaware |\n",
      "| 7d51cb1e-769f-410a-9140-ff2... |                    Jeffrey Orr |                  South Richard |                         Kansas |\n",
      "| 64ffdc55-502b-4e86-9196-426... |                 Rebecca Turner |                    Michaelside |                          Texas |\n",
      "| 4ad12185-8ae6-46fa-8517-293... |                     Alex Roman |                South Loriville |                  Massachusetts |\n",
      "| e14e16a3-528a-47d8-b412-7d7... |              Scott Jimenez PhD |                   Bradleymouth |                     New Mexico |\n",
      "| 466a146c-eb43-4259-be2a-5be... |                Diana Schneider |                      Jessefort |                      Wisconsin |\n",
      "| 061d7e84-4e00-4f32-9f7b-2c3... |                 Amanda Andrews |                   Kevinborough |                       Arkansas |\n",
      "| 8cc07904-63aa-46c1-8839-4f2... |                 Benjamin Jones |                   East Lindsay |                       Illinois |\n",
      "+--------------------------------+--------------------------------+--------------------------------+--------------------------------+\n",
      "10 rows in set\n"
     ]
    }
   ],
   "source": [
    "table_env.execute_sql(f\"SELECT * FROM test_1 \").print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "hudi_output_path = 's3a://danf-s3rm-test/flink/'\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ['AWS_ACCESS_KEY_ID'] = \"AKIAVS5IC62KWW2KWUQ4\"\n",
    "os.environ['AWS_ACCESS_KEY'] = \"AKIAVS5IC62KWW2KWUQ4\"\n",
    "\n",
    "os.environ['AWS_SECRET_ACCESS_KEY'] = \"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\"\n",
    "os.environ['AWS_SECRET_KEY'] = \"AZU3lHXaPFtw/gR23s8Hr+voIEswcrbS7triQnva\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyflink.table.table_result.TableResult at 0x159047b50>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "hudi_sink = f\"\"\"\n",
    "CREATE TABLE customers1(\n",
    "    uuid VARCHAR(36) PRIMARY KEY NOT ENFORCED,\n",
    "    first_name VARCHAR(50),\n",
    "    city VARCHAR(50),\n",
    "    state VARCHAR(50)\n",
    ")\n",
    "PARTITIONED BY (`state`)\n",
    "WITH (\n",
    "    'connector' = 'hudi',\n",
    "    'path' = '{hudi_output_path}' ,\n",
    "    'table.type' = 'COPY_ON_WRITE'\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "# 'write.record-key.field' = 'uuid',\n",
    "# 'write.partition-path.field' = 'state',\n",
    "# 'write.precombine.field' = 'uuid'\n",
    "\n",
    "# Execute the SQL to create the Hudi table\n",
    "table_env.execute_sql(hudi_sink)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-01-26 17:18:36,497 WARN  org.apache.hadoop.metrics2.impl.MetricsConfig                [] - Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties\n",
      "2024-01-26 17:18:36,502 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl            [] - Scheduled Metric snapshot period at 10 second(s).\n",
      "2024-01-26 17:18:36,502 INFO  org.apache.hadoop.metrics2.impl.MetricsSystemImpl            [] - s3a-file-system metrics system started\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o8.executeSql.\n: org.apache.flink.table.api.ValidationException: Unable to create a sink for writing table 'default_catalog.default_database.customers1'.\n\nTable options are:\n\n'connector'='hudi'\n'path'='s3a://danf-s3rm-test/flink/'\n'table.type'='COPY_ON_WRITE'\n\tat org.apache.flink.table.factories.FactoryUtil.createDynamicTableSink(FactoryUtil.java:270)\n\tat org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:459)\n\tat org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:236)\n\tat org.apache.flink.table.planner.delegation.PlannerBase.$anonfun$translate$1(PlannerBase.scala:194)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)\n\tat scala.collection.Iterator.foreach(Iterator.scala:937)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:937)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1425)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:70)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:69)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:54)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:233)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:226)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:194)\n\tat org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1805)\n\tat org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:881)\n\tat org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:991)\n\tat org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:765)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)\n\tat org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.hudi.exception.HoodieIOException: Get table config error\n\tat org.apache.hudi.util.StreamerUtil.getTableConfig(StreamerUtil.java:321)\n\tat org.apache.hudi.table.HoodieTableFactory.setupTableOptions(HoodieTableFactory.java:114)\n\tat org.apache.hudi.table.HoodieTableFactory.createDynamicTableSink(HoodieTableFactory.java:102)\n\tat org.apache.flink.table.factories.FactoryUtil.createDynamicTableSink(FactoryUtil.java:267)\n\t... 29 more\nCaused by: java.nio.file.AccessDeniedException: s3a://danf-s3rm-test/flink/.hoodie/hoodie.properties: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by TemporaryAWSCredentialsProvider SimpleAWSCredentialsProvider EnvironmentVariableCredentialsProvider IAMInstanceCredentialsProvider : com.amazonaws.SdkClientException: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY))\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:212)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:175)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3799)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$exists$34(S3AFileSystem.java:4703)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:4701)\n\tat org.apache.hudi.util.StreamerUtil.getTableConfig(StreamerUtil.java:317)\n\t... 32 more\nCaused by: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by TemporaryAWSCredentialsProvider SimpleAWSCredentialsProvider EnvironmentVariableCredentialsProvider IAMInstanceCredentialsProvider : com.amazonaws.SdkClientException: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY))\n\tat org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:216)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.getCredentialsFromContext(AmazonHttpClient.java:1269)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.runBeforeRequestHandlers(AmazonHttpClient.java:845)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:794)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456)\n\tat com.amazonaws.services.s3.AmazonS3Client.getBucketRegionViaHeadRequest(AmazonS3Client.java:6432)\n\tat com.amazonaws.services.s3.AmazonS3Client.fetchRegionFromCache(AmazonS3Client.java:6404)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5441)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1372)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$10(S3AFileSystem.java:2545)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:414)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:377)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2533)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2513)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3776)\n\t... 40 more\nCaused by: com.amazonaws.SdkClientException: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY))\n\tat com.amazonaws.auth.EnvironmentVariableCredentialsProvider.getCredentials(EnvironmentVariableCredentialsProvider.java:49)\n\tat org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:177)\n\t... 61 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Define the data to be inserted into the Hudi table\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtable_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;43m    INSERT INTO customers1\u001b[39;49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124;43m        SELECT * FROM test_1\u001b[39;49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mwait()\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.17/envs/flink-3.8/lib/python3.8/site-packages/pyflink/table/table_environment.py:837\u001b[0m, in \u001b[0;36mTableEnvironment.execute_sql\u001b[0;34m(self, stmt)\u001b[0m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;124;03mExecute the given single statement, and return the execution result.\u001b[39;00m\n\u001b[1;32m    825\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    834\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.11.0\u001b[39;00m\n\u001b[1;32m    835\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    836\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_before_execute()\n\u001b[0;32m--> 837\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m TableResult(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_j_tenv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecuteSql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstmt\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.17/envs/flink-3.8/lib/python3.8/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.17/envs/flink-3.8/lib/python3.8/site-packages/pyflink/util/exceptions.py:146\u001b[0m, in \u001b[0;36mcapture_java_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw):\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 146\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    148\u001b[0m         \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyflink\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mjava_gateway\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_gateway\n",
      "File \u001b[0;32m~/.pyenv/versions/3.8.17/envs/flink-3.8/lib/python3.8/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o8.executeSql.\n: org.apache.flink.table.api.ValidationException: Unable to create a sink for writing table 'default_catalog.default_database.customers1'.\n\nTable options are:\n\n'connector'='hudi'\n'path'='s3a://danf-s3rm-test/flink/'\n'table.type'='COPY_ON_WRITE'\n\tat org.apache.flink.table.factories.FactoryUtil.createDynamicTableSink(FactoryUtil.java:270)\n\tat org.apache.flink.table.planner.delegation.PlannerBase.getTableSink(PlannerBase.scala:459)\n\tat org.apache.flink.table.planner.delegation.PlannerBase.translateToRel(PlannerBase.scala:236)\n\tat org.apache.flink.table.planner.delegation.PlannerBase.$anonfun$translate$1(PlannerBase.scala:194)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:233)\n\tat scala.collection.Iterator.foreach(Iterator.scala:937)\n\tat scala.collection.Iterator.foreach$(Iterator.scala:937)\n\tat scala.collection.AbstractIterator.foreach(Iterator.scala:1425)\n\tat scala.collection.IterableLike.foreach(IterableLike.scala:70)\n\tat scala.collection.IterableLike.foreach$(IterableLike.scala:69)\n\tat scala.collection.AbstractIterable.foreach(Iterable.scala:54)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:233)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:226)\n\tat scala.collection.AbstractTraversable.map(Traversable.scala:104)\n\tat org.apache.flink.table.planner.delegation.PlannerBase.translate(PlannerBase.scala:194)\n\tat org.apache.flink.table.api.internal.TableEnvironmentImpl.translate(TableEnvironmentImpl.java:1805)\n\tat org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:881)\n\tat org.apache.flink.table.api.internal.TableEnvironmentImpl.executeInternal(TableEnvironmentImpl.java:991)\n\tat org.apache.flink.table.api.internal.TableEnvironmentImpl.executeSql(TableEnvironmentImpl.java:765)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.apache.flink.api.python.shaded.py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat org.apache.flink.api.python.shaded.py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat org.apache.flink.api.python.shaded.py4j.Gateway.invoke(Gateway.java:282)\n\tat org.apache.flink.api.python.shaded.py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat org.apache.flink.api.python.shaded.py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat org.apache.flink.api.python.shaded.py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\nCaused by: org.apache.hudi.exception.HoodieIOException: Get table config error\n\tat org.apache.hudi.util.StreamerUtil.getTableConfig(StreamerUtil.java:321)\n\tat org.apache.hudi.table.HoodieTableFactory.setupTableOptions(HoodieTableFactory.java:114)\n\tat org.apache.hudi.table.HoodieTableFactory.createDynamicTableSink(HoodieTableFactory.java:102)\n\tat org.apache.flink.table.factories.FactoryUtil.createDynamicTableSink(FactoryUtil.java:267)\n\t... 29 more\nCaused by: java.nio.file.AccessDeniedException: s3a://danf-s3rm-test/flink/.hoodie/hoodie.properties: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by TemporaryAWSCredentialsProvider SimpleAWSCredentialsProvider EnvironmentVariableCredentialsProvider IAMInstanceCredentialsProvider : com.amazonaws.SdkClientException: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY))\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:212)\n\tat org.apache.hadoop.fs.s3a.S3AUtils.translateException(S3AUtils.java:175)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3799)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.innerGetFileStatus(S3AFileSystem.java:3688)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$exists$34(S3AFileSystem.java:4703)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.lambda$trackDurationOfOperation$5(IOStatisticsBinding.java:499)\n\tat org.apache.hadoop.fs.statistics.impl.IOStatisticsBinding.trackDuration(IOStatisticsBinding.java:444)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2337)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.trackDurationAndSpan(S3AFileSystem.java:2356)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.exists(S3AFileSystem.java:4701)\n\tat org.apache.hudi.util.StreamerUtil.getTableConfig(StreamerUtil.java:317)\n\t... 32 more\nCaused by: org.apache.hadoop.fs.s3a.auth.NoAuthWithAWSException: No AWS Credentials provided by TemporaryAWSCredentialsProvider SimpleAWSCredentialsProvider EnvironmentVariableCredentialsProvider IAMInstanceCredentialsProvider : com.amazonaws.SdkClientException: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY))\n\tat org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:216)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.getCredentialsFromContext(AmazonHttpClient.java:1269)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.runBeforeRequestHandlers(AmazonHttpClient.java:845)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:794)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n\tat com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n\tat com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5456)\n\tat com.amazonaws.services.s3.AmazonS3Client.getBucketRegionViaHeadRequest(AmazonS3Client.java:6432)\n\tat com.amazonaws.services.s3.AmazonS3Client.fetchRegionFromCache(AmazonS3Client.java:6404)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5441)\n\tat com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5403)\n\tat com.amazonaws.services.s3.AmazonS3Client.getObjectMetadata(AmazonS3Client.java:1372)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.lambda$getObjectMetadata$10(S3AFileSystem.java:2545)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:414)\n\tat org.apache.hadoop.fs.s3a.Invoker.retryUntranslated(Invoker.java:377)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2533)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.getObjectMetadata(S3AFileSystem.java:2513)\n\tat org.apache.hadoop.fs.s3a.S3AFileSystem.s3GetFileStatus(S3AFileSystem.java:3776)\n\t... 40 more\nCaused by: com.amazonaws.SdkClientException: Unable to load AWS credentials from environment variables (AWS_ACCESS_KEY_ID (or AWS_ACCESS_KEY) and AWS_SECRET_KEY (or AWS_SECRET_ACCESS_KEY))\n\tat com.amazonaws.auth.EnvironmentVariableCredentialsProvider.getCredentials(EnvironmentVariableCredentialsProvider.java:49)\n\tat org.apache.hadoop.fs.s3a.AWSCredentialProviderList.getCredentials(AWSCredentialProviderList.java:177)\n\t... 61 more\n"
     ]
    }
   ],
   "source": [
    "# Define the data to be inserted into the Hudi table\n",
    "table_env.execute_sql(\"\"\"\n",
    "    INSERT INTO customers1\n",
    "        SELECT * FROM test_1\n",
    "\"\"\").wait()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flink-3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
