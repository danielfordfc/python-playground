import avro.schema
import fastavro
import tempfile
import os

import datetime as dt

from data_generator import generate_fake_data
import json

from avro.schema import *

import glob


"""
The test performs the following steps:

Iterates over the provided Avro schemas.
Generates fake data using the generate_fake_data function for each schema.
Prepares a new schema with the fully qualified name and no namespace field (qualified_schema).
Serializes the fake data to Avro binary format using the prepared qualified schema (fastavro.writer).
Reads the serialized Avro message back in (fastavro.reader) and checks that its schema matches the original schema.
Compares the deserialized data to the generated fake data to ensure they match.
The test checks if the fake data generated by your function is consistent with the schema and can be
 successfully serialized
and deserialized using the Avro format. If there's any inconsistency between the generated data and the schema 
 or if the data cannot be serialized/deserialized properly, the test will fail.
"""


def process_schema_types(schema_type):
    if isinstance(schema_type, MappingProxyType):
        schema_type = dict(schema_type)
        if schema_type.get('type') == 'enum':
            schema_type['symbols'] = list(schema_type['symbols'])
    elif isinstance(schema_type, list):
        for idx, item in enumerate(schema_type):
            schema_type[idx] = process_schema_types(item)
    elif isinstance(schema_type, dict):
        for key, value in schema_type.items():
            schema_type[key] = process_schema_types(value)
    return schema_type


def datetime_to_timestamp(dt):
    return int(dt.timestamp() * 1000)


def fit_expected_schema(qualified_schema):
    expected_schema = { "name": f"{qualified_schema.namespace}.{qualified_schema.name}" if qualified_schema.namespace else qualified_schema.name,
                "type": qualified_schema.type,
                        'fields': []}

    def process_schema_types(schema_type):
        if isinstance(schema_type, MappingProxyType):
            schema_type = dict(schema_type)
            if schema_type.get('type') == 'enum':
                schema_type['symbols'] = list(schema_type['symbols'])
        elif isinstance(schema_type, list):
            for idx, item in enumerate(schema_type):
                schema_type[idx] = process_schema_types(item)
        elif isinstance(schema_type, dict):
            if 'name' in schema_type and 'namespace' in schema_type:
                schema_type['name'] = f"{schema_type['namespace']}.{schema_type['name']}"
                del schema_type['namespace']
            if schema_type.get('type') == 'enum':
                schema_type['symbols'] = list(schema_type['symbols'])
            for key, value in schema_type.items():
                schema_type[key] = process_schema_types(value)
        return schema_type

    for field in qualified_schema.fields:
        field_json = field.to_json()
        field_type = field_json['type']
        if isinstance(field_type, MappingProxyType):
            field_type = dict(field_type)
        field_type = process_schema_types(field_type)
        field_json['type'] = field_type
        expected_schema['fields'].append(field_json)

    return expected_schema


def test_generate_fake_data():
    # Define the path to the Avro schemas
    path = os.path.join(os.path.dirname(os.path.abspath(__file__)), "schemas/inputs")

    # Load all the Avro schemas in the "inputs" folder
    schema_files = glob.glob(os.path.join(path, "*.avsc"))
    schemas = [avro.schema.parse(open(file, "rb").read()) for file in schema_files]

    for schema in schemas:

        fake_data = generate_fake_data(schema)

        # Create a new schema with the fully qualified name and no namespace field
        qualified_schema = RecordSchema(
            name=f'{schema.name}',
            namespace=schema.namespace,
            fields=schema.fields,
            names=Names()
        )

        qualified_schema_json = qualified_schema.to_json()

        for field in qualified_schema_json['fields']:
            field['type'] = process_schema_types(field['type'])

        output_dumps = ""
        dumps_list = qualified_schema_json['fields']
        for item in dumps_list:
            output_dumps += json.dumps(item)

        # Serialize the fake data to Avro binary format
        with tempfile.NamedTemporaryFile(delete=False) as f:
            fastavro.writer(f, qualified_schema_json, [fake_data])

            f.flush()
            # Read the serialized Avro message back in and check that its schema matches the original schema
            f.seek(0)
            reader = fastavro.reader(f)

            # when namespaces arent handled in the fake_data_generator, then schema.to_json() contains namespace field,
            # that isn't present in the writer_schema. to_canonical_json seems to work though

            #holy mother of god this is a pain to remove the namespaces...
            expected_schema = fit_expected_schema(qualified_schema)

            #RHS = expected_schema
            #LHS = reader.writer_schema
            assert expected_schema == reader.writer_schema

            # Check that the data in the serialized Avro message matches the generated fake data
            deserialized_data = next(reader)

            for key, value in deserialized_data.items():
                if isinstance(value, dt.datetime):
                    deserialized_data[key] = datetime_to_timestamp(value)

            assert deserialized_data == fake_data
