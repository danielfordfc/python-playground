{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandera\n",
    "# !pip install pandas\n",
    "# !pip install hypothesis\n",
    "\n",
    "# !pip install pandera[hypotheses]  \n",
    "#     # hypothesis checks\n",
    "# !pip install pandera[io]          \n",
    "#     # yaml/script schema io utilities\n",
    "# !pip install pandera[strategies]  \n",
    "    # data synthesis strategies\n",
    "# pip install pandera[mypy]        # enable static type-linting of pandas\n",
    "# pip install pandera[fastapi]     # fastapi integration\n",
    "# pip install pandera[dask]        # validate dask dataframes\n",
    "# pip install pandera[pyspark]     # validate pyspark dataframes\n",
    "# pip install pandera[modin]       # validate modin dataframes\n",
    "# pip install pandera[modin-ray]   # validate modin dataframes with ray\n",
    "# pip install pandera[modin-dask]  # validate modin dataframes with dask\n",
    "# pip install pandera[geopandas]   # validate geopandas geodataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandera as pa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrameSchema PARAMETERS\n",
    "- **columns** (mapping of column names and column schema component.) – a dict where keys are column names and values are Column objects specifying the datatypes and properties of a particular column.\n",
    "- **checks** (Optional[CheckList]) – dataframe-wide checks.\n",
    "- **index** – specify the datatypes and properties of the index.\n",
    "- **dtype** (PandasDtypeInputTypes) – datatype of the dataframe. This overrides the data types specified in any of the columns. If a string is specified, then assumes one of the valid pandas string values: http://pandas.pydata.org/pandas-docs/stable/basics.html#dtypes.\n",
    "- **coerce** (bool) – whether or not to coerce all of the columns on validation. This overrides any coerce setting at the column or index level. This has no effect on columns where dtype=None.\n",
    "- **strict** (StrictType) – ensure that all and only the columns defined in the schema are present in the dataframe. If set to ‘filter’, only the columns in the schema will be passed to the validated dataframe. If set to filter and columns defined in the schema are not present in the dataframe, will throw an error.\n",
    "- **name** (Optional[str]) – name of the schema.\n",
    "- **ordered** (bool) – whether or not to validate the columns order.\n",
    "- **unique** (Optional[Union[str, List[str]]]) – a list of columns that should be jointly unique.\n",
    "- **report_duplicates** (UniqueSettings) – how to report unique errors - exclude_first: report all duplicates except first occurence - exclude_last: report all duplicates except last occurence - all: (default) report all duplicates\n",
    "- **unique_column_names** (bool) – whether or not column names must be unique.\n",
    "- **add_missing_columns** (bool) – add missing column names with either default value, if specified in column schema, or NaN if column is nullable.\n",
    "- **title** (Optional[str]) – A human-readable label for the schema.\n",
    "- **description** (Optional[str]) – An arbitrary textual description of the schema.\n",
    "- **metadata** (Optional[dict]) – An optional key-value data.\n",
    "- **drop_invalid_rows** (bool) – if True, drop invalid rows on validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data to validate\n",
    "df = pd.DataFrame({\n",
    "    \"column1\": [1, 4, 0, 10, 9],\n",
    "    \"column2\": [-1.3, -1.4, -2.9, -10.1, -20.4],\n",
    "    \"column3\": [\"value_1\", \"value_2\", \"value_3\", \"value_2\", \"value_1\"],\n",
    "})\n",
    "\n",
    "# define schema with inline checks\n",
    "schema = pa.DataFrameSchema({\n",
    "    \"column1\": pa.Column(int, checks=pa.Check.le(10)),\n",
    "    \"column2\": pa.Column(float, checks=pa.Check.lt(-1.2)),\n",
    "    \"column3\": pa.Column(str, checks=[\n",
    "        pa.Check.str_startswith(\"value_\"),\n",
    "        # define custom checks as functions that take a series as input and\n",
    "        # outputs a boolean or boolean Series\n",
    "        pa.Check(lambda s: s.str.split(\"_\", expand=True).shape[1] == 2)\n",
    "    ]),\n",
    "})\n",
    "\n",
    "# This works, because the checks pass the schema validation for all columns\n",
    "validated_df = schema(df)\n",
    "print(validated_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This will raise a SchemaError, because the checks fail for column2 and 3\n",
    "df_invalid = pd.DataFrame({\n",
    "    \"column1\": [1, 4, 0, 10, 9],\n",
    "    \"column2\": [6, -1.4, -2.9, 10.1, -20.4],\n",
    "    \"column3\": [\"value_1\", \"value_2\", \"value_3\", \"value_2\", \"value_1\"],\n",
    "})\n",
    "\n",
    "# Have to catch the exception to make the output more readable\n",
    "try:\n",
    "    schema(df_invalid)\n",
    "except pa.errors.SchemaError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These schemas, and their checks can be serialized to and from JSON\n",
    "#https://pandera.readthedocs.io/en/stable/schema_inference.html?highlight=schema%20json\n",
    "\n",
    "schema_from_json = pa.DataFrameSchema.from_json(\"schema.json\")\n",
    "\n",
    "schema_from_json\n",
    "\n",
    "# Add a new column, with checks to the schema\n",
    "schema_from_json_fourth_col = schema_from_json.add_columns({\n",
    "    \"column4\": pa.Column(int, checks=pa.Check.ge(0))\n",
    "})\n",
    "\n",
    "# This could serve as a powerful way to dynamically manage read and write schemas in a data pipeline.\n",
    "schema_from_json_fourth_col.to_json(\"schema_with_column4.json\")\n",
    "\n",
    "# You could then validate the dataframe with the new schema, and write on validation success only.\n",
    "# This will fail, as the dataframe does not have the column4\n",
    "schema_from_json_fourth_col.validate(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YAML Schema Example with defined reader and writer schemas\n",
    "\n",
    "- In this example, we will set a reader and a writer schema, defined in YAML files.\n",
    "- We validate the pre-defined schemas against the dataframes before reading and writing.\n",
    "- This is a powerful way to ensure that the data is always in the expected format \n",
    "    and sudden changes in the data wont break the pipeline.\n",
    "\n",
    "In this fictional example, we are collecting data from colleagues at a company, and we want to ensure:\n",
    "\n",
    "- Each employee has a unique ID and can only appear once in the dataset\n",
    "- Their age is over 18 (they've been hired legally)\n",
    "\n",
    "This questionnare was to check who has the longest tenure at the company as a % of their total life, as we have no record of that.\n",
    "During the process, we will materialise a new column, \"tenure_percentage\" which is the tenure at the company as a % of their total life.\n",
    "We don't want cheating! So we will check that the tenure_percentage is less than 100%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_schema_from_yaml = pa.DataFrameSchema.from_yaml(\"reader_schema.yaml\")\n",
    "reader_schema_from_yaml\n",
    "\n",
    "input_df = pd.DataFrame({\n",
    "    \"name\": ['Alice', 'Bob', 'Charlie', 'Dennis', 'Edith'],\n",
    "    \"age\": [25, 32, 29, 19, 22],\n",
    "    \"tenure\": [3, 5, 2, 1, 4]\n",
    "})\n",
    "\n",
    "reader_schema_from_yaml.validate(input_df)\n",
    "\n",
    "#materialise the tenure_percentage column\n",
    "input_df[\"tenure_percentage\"] = (input_df[\"tenure\"] / input_df[\"age\"] * 100)\n",
    "input_df\n",
    "\n",
    "# To ensure that we are writing the expected data, we can validate the dataframe against the writer schema\n",
    "writer_schema_from_yaml = pa.DataFrameSchema.from_yaml(\"writer_schema.yaml\")\n",
    "\n",
    "writer_schema_from_yaml.validate(input_df)\n",
    "\n",
    "input_df.to_csv(\"employee_tenure.csv\", index=False)\n",
    "\n",
    "# Edith wins, with 18.18% of her life spent at the company."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Synthesis\n",
    "\n",
    "https://pandera.readthedocs.io/en/stable/data_synthesis_strategies.html\n",
    "\n",
    "pandera provides a utility for generating synthetic data purely from pandera schema or schema component objects. Under the hood, the schema metadata is collected to create a data-generating strategy using hypothesis, which is a property-based testing library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column1</th>\n",
       "      <th>column2</th>\n",
       "      <th>column3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td>2010-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td>2010-01-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td></td>\n",
       "      <td>2010-01-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   column1 column2    column3\n",
       "0        5         2010-01-01\n",
       "0        5         2010-01-01\n",
       "0        5         2010-01-01"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load that JSON schema into a DataFrameSchema object again\n",
    "\n",
    "schema_from_json = pa.DataFrameSchema.from_json(\"schema.json\")\n",
    "\n",
    "# generate a synthetic dataframe that conforms to the schema\n",
    "\n",
    "synthetic_df = schema_from_json.example(3)\n",
    "synthetic_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
