{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pandera DataFrame validation follow-along workshop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a venv and activate it for isolation\n",
    "# !pyenv versions\n",
    "# !pyenv virtualenv 3.8.17 pandera-3.8.17\n",
    "# !pyenv activate pandera-3.8.17\n",
    "\n",
    "# or without pyenv-virtualenv\n",
    "\n",
    "# !python3 -m venv .venv\n",
    "# !source .venv/bin/activate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas==1.5.3\n",
    "!pip install pyspark==3.3.4 <-- If you want run the pyspark section of the notebook\n",
    "!pip install pandera==0.18.0\n",
    "!pip install hypothesis==6.97.5\n",
    "!pip install pyarrow==15.0.0\n",
    "!pip install pyyaml==6.0\n",
    "!pip install black==21.10b0\n",
    "!pip install frictionless==4.40.8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Pandera?\n",
    "\n",
    "Pandera is a validation library for pandas DataFrames, ensuring data quality by defining and enforcing schemas. It checks for type mismatches, missing values, and data inconsistencies, offering explicit schema validation and documentation for reliable data pipelines.\n",
    "\n",
    "## What problems do we face at FundingCircle?\n",
    "\n",
    "At FundingCircle, we deal with:\n",
    "\n",
    "- Schema drifts due to changes in data sources and loose contracts between producers of data such as internal apps or external endpoints.\n",
    "- Data quality issues that may affect analyses and decision-making processes due to an \n",
    "- Inefficient debugging due to lack of early error detection.\n",
    "- No version-controlled schema management, making it difficult to track and manage schema changes over time for datasets outside the DBT ecosystem.\n",
    "\n",
    "## How can Pandera help us?\n",
    "\n",
    "### Consistency and Quality\n",
    "Pandera allows for explicit schema definition and validation, ensuring data consistency and quality across Python and Spark environments. This reduces schema drift and improves reliability, putting pipelines in a better position to handle data changes on our terms.\n",
    "\n",
    "### Schema on Read and Write\n",
    "With Pandera, we can validate data both when reading into our systems (schema on read) and before writing out (schema on write), ensuring that our data pipelines handle only data that meets our strict criteria.\n",
    "\n",
    "### Version-Controlled Schema Management\n",
    "Pandera supports version-controlled schema management, both internally and simply through definition in a repo + github source control, allowing us to track and manage schema changes over time. This facilitates better governance, compliance, and the ability to rollback to previous schema versions when needed.\n",
    "\n",
    "### Efficiency in Python and Spark\n",
    "Pandera's integration with pandas and the \"Pandas on Spark\" ensures efficient schema validations within our data processing workflows, enhancing developer productivity and reducing debugging time. Similarly, shifting to a Pandas on Spark API will add familiarity and ease of use for our data engineers.\n",
    "\n",
    "\n",
    "In summary, Pandera provides a robust solution for enforcing data quality and schema consistency, which is essential for maintaining reliable data pipelines at FundingCircle. Its support for version-controlled schemas and integration with both pandas and Pandas on Spark API makes it a valuable tool for improving our data processing workflows at FC.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandera as pa\n",
    "\n",
    "# data to validate\n",
    "df = pd.DataFrame({\n",
    "    \"column1\": [1, 4, 0, 10, 9],\n",
    "    \"column2\": [-1.3, -1.4, -2.9, -10.1, -20.4],\n",
    "    \"column3\": [\"value_1\", \"value_2\", \"value_3\", \"value_2\", \"value_1\"],\n",
    "})\n",
    "\n",
    "# define schema with inline checks\n",
    "schema = pa.DataFrameSchema({\n",
    "    \"column1\": pa.Column(int, checks=pa.Check.le(10)),\n",
    "    \"column2\": pa.Column(float, checks=pa.Check.lt(-1.2)),\n",
    "    \"column3\": pa.Column(str, checks=[\n",
    "        pa.Check.str_startswith(\"value_\"),\n",
    "        # define custom checks as functions that take a series as input and\n",
    "        # outputs a boolean or boolean Series\n",
    "        pa.Check(lambda s: s.str.split(\"_\", expand=True).shape[1] == 2)\n",
    "    ]),\n",
    "})\n",
    "\n",
    "# This works, because the checks pass the schema validation for all columns\n",
    "validated_df = schema(df)\n",
    "print(validated_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses schema object and imports from previous cell\n",
    "\n",
    "# This will raise a SchemaError, because the checks fail for column2 and column3\n",
    "df_invalid = pd.DataFrame({\n",
    "    \"column1\": [1, 4, 0, 10, 9],\n",
    "    \"column2\": [6, -1.4, -2.9, 10.1, -20.4],\n",
    "    \"column3\": [\"value_1\", \"value_2\", \"value_3\", \"value_2\", \"value_1\"],\n",
    "})\n",
    "\n",
    "# Catching exception here to make the output more readable\n",
    "try:\n",
    "    schema(df_invalid)\n",
    "except pa.errors.SchemaError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrameSchema parameters of interest\n",
    "\n",
    "The DataFrameSchema object is the primary entry point for defining and validating dataframes. It has the following very useful parameters/checks:\n",
    "\n",
    "- **columns** (mapping of column names and column schema component.) – a dict where keys are column names and values are Column objects specifying the datatypes and properties of a particular column.\n",
    "- **checks** (Optional[CheckList]) – dataframe-wide checks.\n",
    "- **index** – specify the datatypes and properties of the index.\n",
    "- **dtype** (PandasDtypeInputTypes) – datatype of the dataframe. This overrides the data types specified in any of the columns. If a string is specified, then assumes one of the valid pandas string values: http://pandas.pydata.org/pandas-docs/stable/basics.html#dtypes.\n",
    "- **coerce** (bool) – whether or not to coerce all of the columns on validation. This overrides any coerce setting at the column or index level. This has no effect on columns where dtype=None.\n",
    "- **strict** (StrictType) – ensure that all and only the columns defined in the schema are present in the dataframe. If set to ‘filter’, only the columns in the schema will be passed to the validated dataframe. If set to filter and columns defined in the schema are not present in the dataframe, will throw an error.\n",
    "- **name** (Optional[str]) – name of the schema.\n",
    "- **ordered** (bool) – whether or not to validate the columns order.\n",
    "- **unique** (Optional[Union[str, List[str]]]) – a list of columns that should be jointly unique.\n",
    "- **report_duplicates** (UniqueSettings) – how to report unique errors - exclude_first: report all duplicates except first occurence - exclude_last: report all duplicates except last occurence - all: (default) report all duplicates\n",
    "- **unique_column_names** (bool) – whether or not column names must be unique.\n",
    "- **add_missing_columns** (bool) – add missing column names with either default value, if specified in column schema, or NaN if column is nullable.\n",
    "- **title** (Optional[str]) – A human-readable label for the schema.\n",
    "- **description** (Optional[str]) – An arbitrary textual description of the schema.\n",
    "- **metadata** (Optional[dict]) – An optional key-value data.\n",
    "- **drop_invalid_rows** (bool) – if True, drop invalid rows on validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading in a static schema\n",
    "\n",
    "This is a simple example of how to read in a static schema and validate a dataframe against it. This schema can be pre-defined and version-controlled in a repo, and read in as needed for validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These schemas, and their checks can be serialized to and from JSON\n",
    "#https://pandera.readthedocs.io/en/stable/schema_inference.html?highlight=schema%20json\n",
    "\n",
    "schema_from_json = pa.DataFrameSchema.from_json(\"schemas/schema.json\")\n",
    "schema_from_json\n",
    "\n",
    "# Add a new column with checks, to the schema\n",
    "schema_from_json_fourth_col = schema_from_json.add_columns({\n",
    "    \"column4\": pa.Column(int, checks=pa.Check.ge(0))\n",
    "})\n",
    "\n",
    "# This could serve as a powerful way to dynamically manage read and write schemas in a data pipeline.\n",
    "# More simply, you could just use this as a validation tool for your dataframee transformations.\n",
    "schema_from_json_fourth_col.to_json(\"schema_with_column4.json\")\n",
    "\n",
    "# You could then validate the dataframe with the new schema, and write on validation success only.\n",
    "schema_from_json_fourth_col.validate(df)\n",
    "# The above will fail, as the dataframe does not have the column4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## YAML Schema Example with defined reader and writer schemas\n",
    "\n",
    "In this example, we will set a reader and a writer schema, defined in YAML files. We will validate the pre-defined schemas against the dataframes before reading and writing. This is a powerful way to ensure that a multitude of unintended changes in the data or logic changes, wont break the pipeline.\n",
    "\n",
    "In this fictional example, we are collecting data from colleagues at a company, and we want to ensure:\n",
    "\n",
    "1. Each employee has a unique ID and can only appear once in the dataset\n",
    "2. Their age is over 18 (they've been hired legally)\n",
    "\n",
    "This questionnare was to check who has the longest tenure at the company as a % of their total life, as this company has no record of that. During the process, we will materialise a new column, `tenure_percentage` which is the tenure at the company as a % of their total life.\n",
    "\n",
    "We don't want cheating! So we will check that the tenure_percentage is less than 100%. In this fictional pipeline, we would have alerting set up to handle the failure of the data validation checks to ensure the cheater is caught and executed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_schema_from_yaml = pa.DataFrameSchema.from_yaml(\"schemas/reader_schema.yaml\")\n",
    "reader_schema_from_yaml\n",
    "\n",
    "input_df = pd.DataFrame({\n",
    "    \"name\": ['Alice', 'Bob', 'Charlie', 'Dennis', 'Edith'],\n",
    "    \"age\": [25, 32, 29, 19, 22],\n",
    "    \"tenure\": [3, 5, 2, 1, 4]\n",
    "})\n",
    "\n",
    "reader_schema_from_yaml.validate(input_df)\n",
    "\n",
    "#materialise the tenure_percentage column\n",
    "input_df[\"tenure_percentage\"] = (input_df[\"tenure\"] / input_df[\"age\"] * 100)\n",
    "input_df\n",
    "\n",
    "# To ensure that we are writing the expected data, we can validate the dataframe against the writer schema\n",
    "writer_schema_from_yaml = pa.DataFrameSchema.from_yaml(\"schemas/writer_schema.yaml\")\n",
    "\n",
    "writer_schema_from_yaml.validate(input_df)\n",
    "\n",
    "input_df.to_csv(\"employee_tenure.csv\", index=False)\n",
    "\n",
    "# Edith wins, with 18.18% of her life spent at the company... but what does she have to show for it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Synthesis\n",
    "\n",
    "https://pandera.readthedocs.io/en/stable/data_synthesis_strategies.html\n",
    "\n",
    "pandera provides a utility for generating synthetic data purely from pandera schema or schema component objects. Under the hood, the schema metadata is collected to create a data-generating strategy using hypothesis, which is a property-based testing library.\n",
    "\n",
    "This could provide very valuable for testing and development purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load that JSON schema into a DataFrameSchema object again\n",
    "\n",
    "schema_from_json = pa.DataFrameSchema.from_json(\"schemas/schema.json\")\n",
    "\n",
    "# generate a synthetic dataframe that conforms to the schema\n",
    "\n",
    "synthetic_df = schema_from_json.example(3)\n",
    "synthetic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Integration\n",
    "\n",
    "At FundingCircle, we often do larger data processing tasks in Spark, and we would like to validate the data in the same way as we do in Pandas, using the \"Pandas on Spark\" API. Pandera supports this, and the same concepts as above all apply, but the syntax is slightly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ensure Java is installed, if not, install it for this local demo.\n",
    "!java -version\n",
    "# openjdk version \"1.8.0_362\"\n",
    "# OpenJDK Runtime Environment (Zulu 8.68.0.21-CA-macos-aarch64) (build 1.8.0_362-b09)\n",
    "# OpenJDK 64-Bit Server VM (Zulu 8.68.0.21-CA-macos-aarch64) (build 25.362-b09, mixed mode) <-- I believe Java 8 or 11 is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ensure SPARK version matches the pyspark version installed.\n",
    "# This guide assumes you have installed pyspark 3.3.2/4 and spark 3.3.2/4 as the dependencies at the top were tested with these versions.\n",
    "!pyspark --version\n",
    "# Welcome to\n",
    "#       ____              __\n",
    "#      / __/__  ___ _____/ /__\n",
    "#     _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "#    /__ / .__/\\_,_/_/ /_/\\_\\   version 3.3.2\n",
    "#       /_/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps\n",
    "import pandas as pd\n",
    "import pandera as pa\n",
    "\n",
    "from pandera.typing.pyspark import DataFrame, Series\n",
    "\n",
    "# You can define your schema using the pandera DataFrameModel class.\n",
    "class Schema(pa.DataFrameModel):\n",
    "    state: Series[str] = pa.Field(isin=[\"FL\", \"CA\"])\n",
    "    city: Series[str] = pa.Field()\n",
    "    price: Series[int] = pa.Field(in_range={\"min_value\": 5, \"max_value\": 20})\n",
    "\n",
    "Schema.to_yaml(\"schemas/pyspark_schema.yaml\") # <--- if you want to persist the schema you create in this way to YAML.\n",
    "\n",
    "# create a pyspark.pandas dataframe that's validated on object initialization.\n",
    "# This is a more efficient, inline process than using the .validate() method on a dataframe.\n",
    "df = DataFrame[Schema](\n",
    "    {\n",
    "        'state': ['FL','FL','FL','CA','CA','CA'],\n",
    "        'city': [\n",
    "            'Orlando',\n",
    "            'Miami',\n",
    "            'Tampa',\n",
    "            'San Francisco',\n",
    "            'Los Angeles',\n",
    "            'San Diego',\n",
    "        ],\n",
    "        'price': [5, 12, 10, 16, 20, 18], # <-- Change me to see the validation error\n",
    "    }\n",
    ")\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import a Schema from a YAML for DataFrame Validation\n",
    "\n",
    "Like the previous non-spark example, we can import a schema from a YAML file for DataFrame validation. We can do this on read and write to ensure the data is always in the expected format and following our business or functional rules.\n",
    "\n",
    "We can persist these schemas in version control and use them across different parts of the pipeline, ensuring consistency and reducing the risk of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps\n",
    "import pandas as pd\n",
    "import pandera as pa\n",
    "\n",
    "from pandera.typing.pyspark import DataFrame\n",
    "\n",
    "schema_from_yaml = pa.DataFrameSchema.from_yaml(\"schemas/pyspark_schema.yaml\")\n",
    "\n",
    "# create a pyspark.pandas dataframe that's validated on object initialization\n",
    "df = DataFrame(\n",
    "    {\n",
    "        'state': ['FL','FL','FL','CA','CA','CA'],\n",
    "        'city': [\n",
    "            'Orlando',\n",
    "            'Miami',\n",
    "            'Tampa',\n",
    "            'San Francisco',\n",
    "            'Los Angeles',\n",
    "            'San Diego',\n",
    "        ],\n",
    "        'price': [5, 12, 10, 16, 20, 18], # <-- Change me to see the validation error\n",
    "    }\n",
    ")\n",
    "\n",
    "# validate the dataframe - catching exception for clarity\n",
    "try:\n",
    "    schema_from_yaml.validate(df)\n",
    "except pa.errors.SchemaError as e:\n",
    "    print(e)\n",
    "\n",
    "print(df)\n",
    "type(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark DataFrame data synthesis\n",
    "\n",
    "It doesn't appear as though panderas can be used to generate synthetic data for Spark DataFrames, but you must instead convert the data between pandas and spark, and use the Pandas DataFrame data synthesis methods we used previously. This still provides a powerful way to generate synthetic data for testing and development purposes, but it is not as seamless as the Pandas DataFrame data synthesis methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe in pandas for spark and synthesize data using a schema from pandera\n",
    "\n",
    "import pyspark.pandas as ps\n",
    "import pandas as pd\n",
    "import pandera as pa\n",
    "\n",
    "from pandera.typing.pyspark import DataFrame\n",
    "\n",
    "class Schema(pa.DataFrameModel):\n",
    "    state: Series[str] = pa.Field(isin=[\"FL\", \"CA\"])\n",
    "    city: Series[str] = pa.Field()\n",
    "    price: Series[int] = pa.Field(in_range={\"min_value\": 5, \"max_value\": 20})\n",
    "    \n",
    "\n",
    "synthesized_data = Schema.example()\n",
    "spark_df = ps.from_pandas(synthesized_data)\n",
    "spark_df\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
