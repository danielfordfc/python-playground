{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install a venv and activate it for isolation\n",
    "# !pyenv versions\n",
    "# !pyenv virtualenv 3.8.17 pandera-3.8.17\n",
    "# !pyenv activate pandera-3.8.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas==1.5.3\n",
    "!pip install pyspark==3.3.2\n",
    "!pip install pandera==0.18.0\n",
    "!pip install hypothesis==6.97.5\n",
    "!pip install pyarrow==15.0.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "!pip install pyyaml\n",
    "!pip install black\n",
    "!pip install frictionless\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel.ford/.pyenv/versions/pandera/lib/python3.11/site-packages/pyspark/pandas/__init__.py:49: UserWarning: 'PYARROW_IGNORE_TIMEZONE' environment variable was not set. It is required to set this environment variable to '1' in both driver and executor sides if you use pyarrow>=2.0.0. pandas-on-Spark will set it for you but it does not work if there is a Spark context already launched.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandera as pa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrameSchema PARAMETERS\n",
    "- **columns** (mapping of column names and column schema component.) – a dict where keys are column names and values are Column objects specifying the datatypes and properties of a particular column.\n",
    "- **checks** (Optional[CheckList]) – dataframe-wide checks.\n",
    "- **index** – specify the datatypes and properties of the index.\n",
    "- **dtype** (PandasDtypeInputTypes) – datatype of the dataframe. This overrides the data types specified in any of the columns. If a string is specified, then assumes one of the valid pandas string values: http://pandas.pydata.org/pandas-docs/stable/basics.html#dtypes.\n",
    "- **coerce** (bool) – whether or not to coerce all of the columns on validation. This overrides any coerce setting at the column or index level. This has no effect on columns where dtype=None.\n",
    "- **strict** (StrictType) – ensure that all and only the columns defined in the schema are present in the dataframe. If set to ‘filter’, only the columns in the schema will be passed to the validated dataframe. If set to filter and columns defined in the schema are not present in the dataframe, will throw an error.\n",
    "- **name** (Optional[str]) – name of the schema.\n",
    "- **ordered** (bool) – whether or not to validate the columns order.\n",
    "- **unique** (Optional[Union[str, List[str]]]) – a list of columns that should be jointly unique.\n",
    "- **report_duplicates** (UniqueSettings) – how to report unique errors - exclude_first: report all duplicates except first occurence - exclude_last: report all duplicates except last occurence - all: (default) report all duplicates\n",
    "- **unique_column_names** (bool) – whether or not column names must be unique.\n",
    "- **add_missing_columns** (bool) – add missing column names with either default value, if specified in column schema, or NaN if column is nullable.\n",
    "- **title** (Optional[str]) – A human-readable label for the schema.\n",
    "- **description** (Optional[str]) – An arbitrary textual description of the schema.\n",
    "- **metadata** (Optional[dict]) – An optional key-value data.\n",
    "- **drop_invalid_rows** (bool) – if True, drop invalid rows on validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   column1  column2  column3\n",
      "0        1     -1.3  value_1\n",
      "1        4     -1.4  value_2\n",
      "2        0     -2.9  value_3\n",
      "3       10    -10.1  value_2\n",
      "4        9    -20.4  value_1\n"
     ]
    }
   ],
   "source": [
    "# data to validate\n",
    "df = pd.DataFrame({\n",
    "    \"column1\": [1, 4, 0, 10, 9],\n",
    "    \"column2\": [-1.3, -1.4, -2.9, -10.1, -20.4],\n",
    "    \"column3\": [\"value_1\", \"value_2\", \"value_3\", \"value_2\", \"value_1\"],\n",
    "})\n",
    "\n",
    "# define schema with inline checks\n",
    "schema = pa.DataFrameSchema({\n",
    "    \"column1\": pa.Column(int, checks=pa.Check.le(10)),\n",
    "    \"column2\": pa.Column(float, checks=pa.Check.lt(-1.2)),\n",
    "    \"column3\": pa.Column(str, checks=[\n",
    "        pa.Check.str_startswith(\"value_\"),\n",
    "        # define custom checks as functions that take a series as input and\n",
    "        # outputs a boolean or boolean Series\n",
    "        pa.Check(lambda s: s.str.split(\"_\", expand=True).shape[1] == 2)\n",
    "    ]),\n",
    "})\n",
    "\n",
    "# This works, because the checks pass the schema validation for all columns\n",
    "validated_df = schema(df)\n",
    "print(validated_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Schema Column(name=column2, type=DataType(float64))> failed element-wise validator 0:\n",
      "<Check less_than: less_than(-1.2)>\n",
      "failure cases:\n",
      "   index  failure_case\n",
      "0      0           6.0\n",
      "1      3          10.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# This will raise a SchemaError, because the checks fail for column2 and 3\n",
    "df_invalid = pd.DataFrame({\n",
    "    \"column1\": [1, 4, 0, 10, 9],\n",
    "    \"column2\": [6, -1.4, -2.9, 10.1, -20.4],\n",
    "    \"column3\": [\"value_1\", \"value_2\", \"value_3\", \"value_2\", \"value_1\"],\n",
    "})\n",
    "\n",
    "# Catching exception here to make the output more readable\n",
    "try:\n",
    "    schema(df_invalid)\n",
    "except pa.errors.SchemaError as e:\n",
    "    print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These schemas, and their checks can be serialized to and from JSON\n",
    "#https://pandera.readthedocs.io/en/stable/schema_inference.html?highlight=schema%20json\n",
    "\n",
    "schema_from_json = pa.DataFrameSchema.from_json(\"schemas/schema.json\")\n",
    "schema_from_json\n",
    "\n",
    "# Add a new column with checks, to the schema\n",
    "schema_from_json_fourth_col = schema_from_json.add_columns({\n",
    "    \"column4\": pa.Column(int, checks=pa.Check.ge(0))\n",
    "})\n",
    "\n",
    "# This could serve as a powerful way to dynamically manage read and write schemas in a data pipeline.\n",
    "# More simply, you could just use this as a validation tool for your dataframee transformations.\n",
    "schema_from_json_fourth_col.to_json(\"schema_with_column4.json\")\n",
    "\n",
    "# You could then validate the dataframe with the new schema, and write on validation success only.\n",
    "schema_from_json_fourth_col.validate(df)\n",
    "# The above will fail, as the dataframe does not have the column4\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# YAML Schema Example with defined reader and writer schemas\n",
    "\n",
    "- In this example, we will set a reader and a writer schema, defined in YAML files.\n",
    "- We validate the pre-defined schemas against the dataframes before reading and writing.\n",
    "- This is a powerful way to ensure that the data is always in the expected format\n",
    "    and sudden changes in the data wont break the pipeline.\n",
    "\n",
    "In this fictional example, we are collecting data from colleagues at a company, and we want to ensure:\n",
    "\n",
    "- Each employee has a unique ID and can only appear once in the dataset\n",
    "- Their age is over 18 (they've been hired legally)\n",
    "\n",
    "This questionnare was to check who has the longest tenure at the company as a % of their total life, as we have no record of that.\n",
    "During the process, we will materialise a new column, \"tenure_percentage\" which is the tenure at the company as a % of their total life.\n",
    "We don't want cheating! So we will check that the tenure_percentage is less than 100%.\n",
    "\n",
    "In my fictional pipeline, I would have alerting set up to handle the failure of the data validation checks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_schema_from_yaml = pa.DataFrameSchema.from_yaml(\"schemas/reader_schema.yaml\")\n",
    "reader_schema_from_yaml\n",
    "\n",
    "input_df = pd.DataFrame({\n",
    "    \"name\": ['Alice', 'Bob', 'Charlie', 'Dennis', 'Edith'],\n",
    "    \"age\": [25, 32, 29, 19, 22],\n",
    "    \"tenure\": [3, 5, 2, 1, 4]\n",
    "})\n",
    "\n",
    "reader_schema_from_yaml.validate(input_df)\n",
    "\n",
    "#materialise the tenure_percentage column\n",
    "input_df[\"tenure_percentage\"] = (input_df[\"tenure\"] / input_df[\"age\"] * 100)\n",
    "input_df\n",
    "\n",
    "# To ensure that we are writing the expected data, we can validate the dataframe against the writer schema\n",
    "writer_schema_from_yaml = pa.DataFrameSchema.from_yaml(\"schemas/writer_schema.yaml\")\n",
    "\n",
    "writer_schema_from_yaml.validate(input_df)\n",
    "\n",
    "input_df.to_csv(\"employee_tenure.csv\", index=False)\n",
    "\n",
    "# Edith wins, with 18.18% of her life spent at the company."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Synthesis\n",
    "\n",
    "https://pandera.readthedocs.io/en/stable/data_synthesis_strategies.html\n",
    "\n",
    "pandera provides a utility for generating synthetic data purely from pandera schema or schema component objects. Under the hood, the schema metadata is collected to create a data-generating strategy using hypothesis, which is a property-based testing library.\n",
    "\n",
    "This could provide very valuable for testing and development purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load that JSON schema into a DataFrameSchema object again\n",
    "\n",
    "schema_from_json = pa.DataFrameSchema.from_json(\"schemas/schema.json\")\n",
    "\n",
    "# generate a synthetic dataframe that conforms to the schema\n",
    "\n",
    "synthetic_df = schema_from_json.example(3)\n",
    "synthetic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Integration\n",
    "\n",
    "At FundingCircle, we often do larger data processing tasks in Spark, and we would like to validate the data in the same way as we do in Pandas, using Pyspark Pandas.\n",
    "\n",
    "The same concepts as above all apply, but the syntax is slightly different."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ensure Java is installed, if not, install it for this local demo.\n",
    "!java -version\n",
    "\n",
    "# openjdk version \"1.8.0_362\"\n",
    "# OpenJDK Runtime Environment (Zulu 8.68.0.21-CA-macos-aarch64) (build 1.8.0_362-b09)\n",
    "# OpenJDK 64-Bit Server VM (Zulu 8.68.0.21-CA-macos-aarch64) (build 25.362-b09, mixed mode)\n",
    "\n",
    "# Ensure SPARK version matches the pyspark version installed\n",
    "!pyspark\n",
    "\n",
    "# Welcome to\n",
    "#       ____              __\n",
    "#      / __/__  ___ _____/ /__\n",
    "#     _\\ \\/ _ \\/ _ `/ __/  '_/\n",
    "#    /__ / .__/\\_,_/_/ /_/\\_\\   version 3.3.2\n",
    "#       /_/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.pandas as ps\n",
    "import pandas as pd\n",
    "import pandera as pa\n",
    "\n",
    "from pandera.typing.pyspark import DataFrame, Series\n",
    "\n",
    "# You can define your schema using the pandera DataFrameModel class using traditional types.\n",
    "class Schema(pa.DataFrameModel):\n",
    "    state: Series[str] = pa.Field(in_set=[\"FL\", \"CA\"])\n",
    "    city: Series[str] = pa.Field(allow_duplicates=False)\n",
    "    price: Series[int] = pa.Field(in_range={\"min_value\": 5, \"max_value\": 20})\n",
    "\n",
    "# Schema.to_yaml(\"schemas/pyspark_schema.yaml\") if you want to persist the schema to YAML.\n",
    "\n",
    "# create a pyspark.pandas dataframe that's validated on object initialization.\n",
    "# This is a more efficient, inline process than using the .validate method on a dataframe.\n",
    "df = DataFrame[Schema](\n",
    "    {\n",
    "        'state': ['FL','FL','FL','CA','CA','CA'],\n",
    "        'city': [\n",
    "            'Orlando',\n",
    "            'Miami',\n",
    "            'Tampa',\n",
    "            'San Francisco',\n",
    "            'Los Angeles',\n",
    "            'San Diego',\n",
    "        ],\n",
    "        'price': [5, 12, 10, 16, 20, 18], # <-- Change me to see the validation error\n",
    "    }\n",
    ")\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import a Schema from a YAML for DataFrame Validation\n",
    "\n",
    "Like the previous non-spark example, we can import a schema from a YAML file for DataFrame validation. We can do this on read and write to ensure the data is always in the expected format and following our business or functional rules.\n",
    "\n",
    "We can persist these schemas in version control and use them across different parts of the pipeline, ensuring consistency and reducing the risk of errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel.ford/.pyenv/versions/3.8.17/envs/pandera-3.8.17/lib/python3.8/site-packages/pyspark/pandas/internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  fields = [\n",
      "/Users/daniel.ford/.pyenv/versions/3.8.17/envs/pandera-3.8.17/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/Users/daniel.ford/.pyenv/versions/3.8.17/envs/pandera-3.8.17/lib/python3.8/site-packages/pyspark/pandas/internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  fields = [\n",
      "/Users/daniel.ford/.pyenv/versions/3.8.17/envs/pandera-3.8.17/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/Users/daniel.ford/spark/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/pandas/internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "/Users/daniel.ford/spark/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/pandas/internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "/Users/daniel.ford/spark/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/pandas/internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "/Users/daniel.ford/spark/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/pandas/internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "/Users/daniel.ford/spark/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/pandas/internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "/Users/daniel.ford/spark/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/pandas/internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "/Users/daniel.ford/spark/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/pandas/internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "/Users/daniel.ford/spark/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/pandas/internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "/Users/daniel.ford/spark/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/pandas/internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "/Users/daniel.ford/spark/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/pandas/internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "/Users/daniel.ford/spark/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/pandas/internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "/Users/daniel.ford/spark/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/pandas/internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "/Users/daniel.ford/.pyenv/versions/3.8.17/envs/pandera-3.8.17/lib/python3.8/site-packages/pyspark/pandas/internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  fields = [\n",
      "/Users/daniel.ford/.pyenv/versions/3.8.17/envs/pandera-3.8.17/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n",
      "/Users/daniel.ford/spark/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/pandas/internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "/Users/daniel.ford/spark/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/pandas/internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "/Users/daniel.ford/spark/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/pandas/internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "/Users/daniel.ford/spark/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/pandas/internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "/Users/daniel.ford/spark/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/pandas/internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "/Users/daniel.ford/spark/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/pandas/internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "/Users/daniel.ford/spark/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/pandas/internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "/Users/daniel.ford/spark/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/pandas/internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "/Users/daniel.ford/spark/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/pandas/internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "/Users/daniel.ford/spark/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/pandas/internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "/Users/daniel.ford/spark/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/pandas/internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "/Users/daniel.ford/spark/spark-3.3.2-bin-hadoop3/python/lib/pyspark.zip/pyspark/pandas/internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  state           city  price\n",
      "0    FL        Orlando      5\n",
      "1    FL          Miami     12\n",
      "2    FL          Tampa     10\n",
      "3    CA  San Francisco     16\n",
      "4    CA    Los Angeles     20\n",
      "5    CA      San Diego     18\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pandera.typing.pyspark.DataFrame"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark.pandas as ps\n",
    "import pandas as pd\n",
    "import pandera as pa\n",
    "\n",
    "from pandera.typing.pyspark import DataFrame\n",
    "\n",
    "schema_from_yaml = pa.DataFrameSchema.from_yaml(\"schemas/pyspark_schema.yaml\")\n",
    "\n",
    "# create a pyspark.pandas dataframe that's validated on object initialization\n",
    "df = DataFrame(\n",
    "    {\n",
    "        'state': ['FL','FL','FL','CA','CA','CA'],\n",
    "        'city': [\n",
    "            'Orlando',\n",
    "            'Miami',\n",
    "            'Tampa',\n",
    "            'San Francisco',\n",
    "            'Los Angeles',\n",
    "            'San Diego',\n",
    "        ],\n",
    "        'price': [5, 12, 10, 16, 20, 18], # <-- Change me to see the validation error\n",
    "    }\n",
    ")\n",
    "\n",
    "# validate the dataframe - catching exception for clarity\n",
    "try:\n",
    "    schema_from_yaml.validate(df)\n",
    "except pa.errors.SchemaError as e:\n",
    "    print(e)\n",
    "\n",
    "print(df)\n",
    "type(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark DataFrame data synthesis\n",
    "\n",
    "We can also use pandera to generate synthetic data for Spark DataFrames. This is a very powerful tool for testing and development purposes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel.ford/.pyenv/versions/3.8.17/envs/pandera-3.8.17/lib/python3.8/site-packages/pyspark/pandas/internal.py:1573: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  fields = [\n",
      "/Users/daniel.ford/.pyenv/versions/3.8.17/envs/pandera-3.8.17/lib/python3.8/site-packages/pyspark/sql/pandas/conversion.py:486: FutureWarning: iteritems is deprecated and will be removed in a future version. Use .items instead.\n",
      "  for column, series in pdf.iteritems():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        state   city  price\n",
      "0       åhÊ^  åhÊ^      7\n",
      "1  Î\\tËÑ򥺻{ L     󖸉\u0006     14\n",
      "2          [d  ùÿ\u001c+\u000e     10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dataframe in pandas for spark and synthesize data using a schema from pandera\n",
    "\n",
    "import pyspark.pandas as ps\n",
    "import pandas as pd\n",
    "import pandera as pa\n",
    "\n",
    "from pandera.typing.pyspark import DataFrame\n",
    "\n",
    "# create a pyspark.pandas dataframe that's validated on object initialization\n",
    "\n",
    "schema_from_yaml = pa.DataFrameSchema.from_yaml(\"schemas/pyspark_schema.yaml\")\n",
    "\n",
    "... todo\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
